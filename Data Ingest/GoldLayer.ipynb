{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "098341c2-4b17-4433-add2-b5f647ea286f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Gold Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a9ffff6-d1ed-4ebb-83c6-6095f70318bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, col\n",
    "\n",
    "# Caminho da camada Silver\n",
    "silver_directory_delta = \"/covid/silver\"\n",
    "\n",
    "# Caminho da camada Gold\n",
    "gold_directory_delta = \"/covid/gold\"\n",
    "\n",
    "# Carregar os dados da camada Silver em formato Delta\n",
    "df_silver = spark.read.format(\"delta\").load(silver_directory_delta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f467003-406d-4220-a025-12259c9da08c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Adicionar coluna de ano para particionamento\n",
    "df_silver_with_year = df_silver.withColumn(\"year\", year(col(\"date\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ea50ddf-76cb-4dda-b66f-7488cb9286cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Renomeando colunas\n",
    "df_gold = df_silver_with_year.withColumnRenamed(\"new_confirmed\", \"daily_new_cases\") \\\n",
    "                   .withColumnRenamed(\"new_deceased\", \"daily_new_deaths\") \\\n",
    "                   .withColumnRenamed(\"cumulative_confirmed\", \"total_confirmed_cases\") \\\n",
    "                   .withColumnRenamed(\"cumulative_deceased\", \"total_confirmed_deaths\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4563fc6b-5e91-471c-a8e8-27eb065b8492",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados particionados por ano e salvos na camada Gold: /covid/gold\n"
     ]
    }
   ],
   "source": [
    "# 3. Escrever os dados na camada Gold, particionando por ano e aplicando ZORDER\n",
    "(df_gold.write\n",
    " .format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .partitionBy(\"year\")\n",
    " .option(\"overwriteSchema\", \"true\")\n",
    " .save(gold_directory_delta))\n",
    "\n",
    "# 4. Aplicar ZORDER BY na coluna 'location_key' para otimizar consultas\n",
    "spark.sql(f\"OPTIMIZE '{gold_directory_delta}' ZORDER BY (date)\")\n",
    "\n",
    "print(f\"Dados particionados por ano e salvos na camada Gold: {gold_directory_delta}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "GoldLayer",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
