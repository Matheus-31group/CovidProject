{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2008769c-471a-4a27-96ce-845bd10d32a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo CSV lido com sucesso no DataFrame Spark!\n",
      "Dados carregados na camada Bronze no formato Delta com sucesso!\n",
      "Formato do arquivo: delta\n",
      "Quantidade de linhas: 22712\n",
      "Quantidade de colunas: 737\n",
      "Arquivo temporário removido do DBFS.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import traceback\n",
    "\n",
    "# Definindo a URL da API\n",
    "url = \"https://storage.googleapis.com/covid19-open-data/v3/latest/aggregated.csv\"\n",
    "dbfs_temp_file_path = \"dbfs:/tmp/aggregated.csv\"\n",
    "\n",
    "try:\n",
    "    # 1. Fazendo a requisição para obter os dados\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Verifica se houve algum erro na requisição\n",
    "\n",
    "    # 2. Salvando o conteúdo em um arquivo temporário CSV no DBFS\n",
    "    with open(\"/tmp/aggregated.csv\", \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "    \n",
    "    # Movendo o arquivo local para o DBFS\n",
    "    dbutils.fs.mv(\"file:/tmp/aggregated.csv\", dbfs_temp_file_path)\n",
    "\n",
    "    # 3. Leitura do arquivo CSV em um DataFrame Spark\n",
    "    df_bronze = spark.read.csv(dbfs_temp_file_path, header=True, inferSchema=True)\n",
    "    \n",
    "    if df_bronze.rdd.isEmpty():\n",
    "        raise ValueError(\"O arquivo CSV está vazio.\")\n",
    "    \n",
    "    print(\"Arquivo CSV lido com sucesso no DataFrame Spark!\")\n",
    "\n",
    "    # 4. Carregamento dos Dados na Camada Bronze em formato Delta\n",
    "    bronze_directory = \"/covid/bronze\" \n",
    "\n",
    "    # Criando o diretório, caso não exista\n",
    "    dbutils.fs.mkdirs(bronze_directory)\n",
    "\n",
    "    # Salvando os dados em formato Delta na camada Bronze\n",
    "    df_bronze.write.format(\"delta\").mode(\"overwrite\").save(bronze_directory)\n",
    "    print(\"Dados carregados na camada Bronze no formato Delta com sucesso!\")\n",
    "\n",
    "    # 5. Leitura dos dados carregados \n",
    "    df_bronze_loaded = spark.read.format(\"delta\").load(bronze_directory)\n",
    "\n",
    "    # Quantidade de linhas e colunas\n",
    "    row_count = df_bronze_loaded.count()\n",
    "    column_count = len(df_bronze_loaded.columns)\n",
    "\n",
    "    print(f\"Quantidade de linhas: {row_count}\")\n",
    "    print(f\"Quantidade de colunas: {column_count}\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Erro ao baixar o arquivo CSV: {e}\")\n",
    "    print(traceback.format_exc())\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao processar o arquivo CSV: {e}\")\n",
    "    print(traceback.format_exc())\n",
    "    raise\n",
    "finally:\n",
    "    # Removendo o arquivo temporário no DBFS\n",
    "    dbutils.fs.rm(dbfs_temp_file_path)\n",
    "    print(\"Arquivo temporário removido do DBFS.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BronzeLayer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
